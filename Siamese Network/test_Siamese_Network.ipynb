{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220025,\n",
       " 57458,\n",
       " 277483,\n",
       " [('f38a6374c348f90b587e046aac6079959adf3835', 0),\n",
       "  ('c18f2d887b7ae4f6742ee445113fa1aef383ed77', 1),\n",
       "  ('755db6279dae599ebb4d39a9123cce439965282d', 0),\n",
       "  ('bc3f0c64fb968ff4a8bd33af6971ecae77c75e08', 0),\n",
       "  ('068aba587a4950175d04c680d38943fd488d6a9d', 0)],\n",
       " ['0b2ea2a822ad23fdb1b5dd26653da899fbd2c0d5',\n",
       "  '95596b92e5066c5c52466c90b69ff089b39f2737',\n",
       "  '248e6738860e2ebcf6258cdc1f32f299e0c76914',\n",
       "  '2c35657e312966e9294eac6841726ff3a748febf',\n",
       "  '145782eb7caa1c516acbe2eda34d9a3f31c41fd6'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "tagged = dict([(p,w) for _,p,w in read_csv('../input/train_labels.csv').to_records()])\n",
    "submit = [p for _,p,_ in read_csv('../input/sample_submission.csv').to_records()]\n",
    "join   = list(tagged.keys()) + submit\n",
    "len(tagged),len(submit),len(join),list(tagged.items())[:5],submit[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=277483), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(277483,\n",
       " [('f38a6374c348f90b587e046aac6079959adf3835', (96, 96)),\n",
       "  ('c18f2d887b7ae4f6742ee445113fa1aef383ed77', (96, 96)),\n",
       "  ('755db6279dae599ebb4d39a9123cce439965282d', (96, 96)),\n",
       "  ('bc3f0c64fb968ff4a8bd33af6971ecae77c75e08', (96, 96)),\n",
       "  ('068aba587a4950175d04c680d38943fd488d6a9d', (96, 96))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os.path import isfile\n",
    "from PIL import Image as pil_image\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def expand_path(p):\n",
    "    if isfile('../input/train/' + p +'.tif'): \n",
    "        #print(\"train\")\n",
    "        return '../input/train/' + p +'.tif'\n",
    "   \n",
    "    if isfile('../input/test/' + p +'.tif'): \n",
    "        #print(\"test\")\n",
    "        return '../input/test/' + p +'.tif'\n",
    "    return p\n",
    "\n",
    "p2size = {}\n",
    "for p in tqdm_notebook(join):\n",
    "    size      = pil_image.open(expand_path(p)).size\n",
    "    p2size[p] = size\n",
    "len(p2size), list(p2size.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=277483), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=277393), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from imagehash import phash\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "def match(h1,h2):\n",
    "    for p1 in h2ps[h1]:\n",
    "        for p2 in h2ps[h2]:\n",
    "            i1 =  pil_image.open(expand_path(p1))\n",
    "            i2 =  pil_image.open(expand_path(p2))\n",
    "            if i1.mode != i2.mode or i1.size != i2.size: return False\n",
    "            a1 = np.array(i1)\n",
    "            a1 = a1 - a1.mean()\n",
    "            a1 = a1/sqrt((a1**2).mean())\n",
    "            a2 = np.array(i2)\n",
    "            a2 = a2 - a2.mean()\n",
    "            a2 = a2/sqrt((a2**2).mean())\n",
    "            a  = ((a1 - a2)**2).mean()\n",
    "            if a > 0.1: return False\n",
    "    return True\n",
    "\n",
    "if isfile('./p2h.pickle'):\n",
    "    with open('./p2h.pickle', 'rb') as f:\n",
    "        p2h = pickle.load(f)\n",
    "else:\n",
    "    \n",
    "    p2h = {}\n",
    "    for p in tqdm_notebook(join):\n",
    "        img    = pil_image.open(expand_path(p))\n",
    "        h      = phash(img)\n",
    "        p2h[p] = h\n",
    "\n",
    "    \n",
    "    h2ps = {}\n",
    "    for p,h in p2h.items():\n",
    "        if h not in h2ps: h2ps[h] = []\n",
    "        if p not in h2ps[h]: h2ps[h].append(p)\n",
    "\n",
    "    \n",
    "    hs = list(h2ps.keys())\n",
    "\n",
    "    \n",
    "    h2h = {}\n",
    "    for i,h1 in enumerate(tqdm_notebook(hs)):\n",
    "        for h2 in hs[:i]:\n",
    "            if h1-h2 <= 6 and match(h1, h2):\n",
    "                s1 = str(h1)\n",
    "                s2 = str(h2)\n",
    "                if s1 < s2: s1,s2 = s2,s1\n",
    "                h2h[s1] = s2\n",
    "\n",
    "    \n",
    "    for p,h in p2h.items():\n",
    "        h = str(h)\n",
    "        if h in h2h: h = h2h[h]\n",
    "        p2h[p] = h\n",
    "        \n",
    "    with open('p2h.pickle', 'wb') as f: \n",
    "        pickle.dump(p2h, f)\n",
    "\n",
    "len(p2h), list(p2h.items())[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h2ps = {}\n",
    "for p,h in p2h.items():\n",
    "    if h not in h2ps: h2ps[h] = []\n",
    "    if p not in h2ps[h]: h2ps[h].append(p)\n",
    "len(h2ps),list(h2ps.items())[:5]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_whale(imgs, per_row=2):\n",
    "    n         = len(imgs)\n",
    "    rows      = (n + per_row - 1)//per_row\n",
    "    cols      = min(per_row, n)\n",
    "    fig, axes = plt.subplots(rows,cols, figsize=(24//per_row*cols,24//per_row*rows))\n",
    "    for ax in axes.flatten(): ax.axis('off')\n",
    "    for i,(img,ax) in enumerate(zip(imgs, axes.flatten())): \n",
    "        ax.imshow(img.convert('RGB'))\n",
    "        plt.show()\n",
    "\n",
    "for h, ps in h2ps.items():\n",
    "    if len(ps) > 2:\n",
    "        print('Images:', ps)\n",
    "        imgs = [pil_image.open(expand_path(p)) for p in ps]\n",
    "        show_whale(imgs, per_row=len(ps))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prefer(ps):\n",
    "    if len(ps) == 1: return ps[0]\n",
    "    best_p = ps[0]\n",
    "    best_s = p2size[best_p]\n",
    "    for i in range(1, len(ps)):\n",
    "        p = ps[i]\n",
    "        s = p2size[p]\n",
    "        if s[0]*s[1] > best_s[0]*best_s[1]: \n",
    "            best_p = p\n",
    "            best_s = s\n",
    "    return best_p\n",
    "\n",
    "h2p = {}\n",
    "for h,ps in h2ps.items(): h2p[h] = prefer(ps)\n",
    "len(h2p),list(h2p.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_raw_image(p):\n",
    "    img = pil_image.open(expand_path(p))    \n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "old_stderr = sys.stderr\n",
    "sys.stderr = open('/dev/null' if platform.system() != 'Windows' else 'nul', 'w')\n",
    "import keras\n",
    "sys.stderr = old_stderr\n",
    "\n",
    "import random\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import img_to_array,array_to_img\n",
    "from scipy.ndimage import affine_transform\n",
    "\n",
    "\n",
    "img_shape    = (96,96,1) \n",
    "anisotropy   = 2.15 \n",
    "crop_margin  = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_transform(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    \n",
    "    rotation        = np.deg2rad(rotation)\n",
    "    shear           = np.deg2rad(shear)\n",
    "    rotation_matrix = np.array([[np.cos(rotation), np.sin(rotation), 0], [-np.sin(rotation), np.cos(rotation), 0], [0, 0, 1]])\n",
    "    shift_matrix    = np.array([[1, 0, height_shift], [0, 1, width_shift], [0, 0, 1]])\n",
    "    shear_matrix    = np.array([[1, np.sin(shear), 0], [0, np.cos(shear), 0], [0, 0, 1]])\n",
    "    zoom_matrix     = np.array([[1.0/height_zoom, 0, 0], [0, 1.0/width_zoom, 0], [0, 0, 1]])\n",
    "    shift_matrix    = np.array([[1, 0, -height_shift], [0, 1, -width_shift], [0, 0, 1]])\n",
    "    return np.dot(np.dot(rotation_matrix, shear_matrix), np.dot(zoom_matrix, shift_matrix))\n",
    "\n",
    "def read_cropped_image(p, augment):\n",
    "   \n",
    "    if p in h2p: p = h2p[p]\n",
    "    size_x,size_y = p2size[p]\n",
    "        \n",
    "    img   = read_raw_image(p).convert('L')\n",
    "    img   = img_to_array(img)   \n",
    "    img    = img.reshape(img_shape)\n",
    "  \n",
    "    img  -= np.mean(img, keepdims=True)\n",
    "    img  /= np.std(img, keepdims=True) + K.epsilon()\n",
    "    return img\n",
    "\n",
    "def read_for_training(p):\n",
    "   \n",
    "    return read_cropped_image(p, True)\n",
    "\n",
    "def read_for_validation(p):\n",
    "   \n",
    "    return read_cropped_image(p, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = list(tagged.keys())[312]\n",
    "imgs = [\n",
    "    read_raw_image(p),\n",
    "    array_to_img(read_for_validation(p)),\n",
    "    array_to_img(read_for_training(p))\n",
    "]\n",
    "show_whale(imgs, per_row=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.engine.topology import Input\n",
    "from keras.layers import Activation, Add, BatchNormalization, Concatenate, Conv2D, Dense, Flatten, GlobalMaxPooling2D, Lambda, MaxPooling2D, Reshape\n",
    "from keras.models import Model\n",
    "\n",
    "def subblock(x, filter, **kwargs):\n",
    "    x = BatchNormalization()(x)\n",
    "    y = x\n",
    "    y = Conv2D(filter, (1, 1), activation='relu', **kwargs)(y) \n",
    "    y = BatchNormalization()(y)\n",
    "    y = Conv2D(filter, (3, 3), activation='relu', **kwargs)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Conv2D(K.int_shape(x)[-1], (1, 1), **kwargs)(y)\n",
    "    y = Add()([x,y]) \n",
    "    y = Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "def build_model(lr, l2, activation='sigmoid'):\n",
    "\n",
    "   \n",
    "    regul  = regularizers.l2(l2)\n",
    "    optim  = Adam(lr=lr)\n",
    "    kwargs = {'padding':'same', 'kernel_regularizer':regul}\n",
    "\n",
    "    inp = Input(shape=img_shape) \n",
    "    x   = Conv2D(64, (9,9), strides=2, activation='relu', **kwargs)(inp)\n",
    "\n",
    "    x   = MaxPooling2D((2, 2), strides=(2, 2))(x) \n",
    "    for _ in range(2):\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2D(64, (3,3), activation='relu', **kwargs)(x)\n",
    "\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, (1,1), activation='relu', **kwargs)(x) \n",
    "    for _ in range(4): x = subblock(x, 64, **kwargs)\n",
    "\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(256, (1,1), activation='relu', **kwargs)(x) \n",
    "    for _ in range(4): x = subblock(x, 64, **kwargs)\n",
    "\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(384, (1,1), activation='relu', **kwargs)(x) \n",
    "    for _ in range(4): x = subblock(x, 96, **kwargs)\n",
    "\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(512, (1,1), activation='relu', **kwargs)(x) \n",
    "    for _ in range(4): x = subblock(x, 128, **kwargs)\n",
    "    \n",
    "    x             = GlobalMaxPooling2D()(x) # 512\n",
    "    branch_model  = Model(inp, x)\n",
    "    \n",
    "   \n",
    "    mid        = 32\n",
    "    xa_inp     = Input(shape=branch_model.output_shape[1:])\n",
    "    xb_inp     = Input(shape=branch_model.output_shape[1:])\n",
    "    x1         = Lambda(lambda x : x[0]*x[1])([xa_inp, xb_inp])\n",
    "    x2         = Lambda(lambda x : x[0] + x[1])([xa_inp, xb_inp])\n",
    "    x3         = Lambda(lambda x : K.abs(x[0] - x[1]))([xa_inp, xb_inp])\n",
    "    x4         = Lambda(lambda x : K.square(x))(x3)\n",
    "    x          = Concatenate()([x1, x2, x3, x4])\n",
    "    x          = Reshape((4, branch_model.output_shape[1], 1), name='reshape1')(x)\n",
    "\n",
    "    \n",
    "    x          = Conv2D(mid, (4, 1), activation='relu', padding='valid')(x)\n",
    "    x          = Reshape((branch_model.output_shape[1], mid, 1))(x)\n",
    "    x          = Conv2D(1, (1, mid), activation='linear', padding='valid')(x)\n",
    "    x          = Flatten(name='flatten')(x)\n",
    "    \n",
    "    \n",
    "    x          = Dense(1, use_bias=True, activation=activation, name='weighted-average')(x)\n",
    "    head_model = Model([xa_inp, xb_inp], x, name='head')\n",
    "    \n",
    "    img_a      = Input(shape=img_shape)\n",
    "    img_b      = Input(shape=img_shape)\n",
    "    xa         = branch_model(img_a)\n",
    "    xb         = branch_model(img_b)\n",
    "    x          = head_model([xa, xb])\n",
    "    model      = Model([img_a, img_b], x)\n",
    "    model.compile(optim, loss='binary_crossentropy', metrics=['binary_crossentropy', 'acc'])\n",
    "  \n",
    "    return model, branch_model, head_model\n",
    "\n",
    "model, branch_model, head_model = build_model(64e-5,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "branch_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "head_model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(head_model, to_file='head-model.png')\n",
    "pil_image.open('head-model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_model(branch_model, to_file='branch-model.png')\n",
    "img = pil_image.open('branch-model.png')\n",
    "img.resize([x//2 for x in img.size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h2ws = {}\n",
    "\n",
    "for p,w in tagged.items():\n",
    "    #if w != new_whale: \n",
    "    h = p2h[p]\n",
    "    if h not in h2ws: h2ws[h] = []\n",
    "    if w not in h2ws[h]: h2ws[h].append(w)\n",
    "for h,ws in h2ws.items():\n",
    "    if len(ws) > 1:\n",
    "        h2ws[h] = sorted(ws)\n",
    "len(h2ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2hs = {}\n",
    "for h,ws in h2ws.items():\n",
    "    if len(ws) == 1: \n",
    "        #if h2p[h] in exclude:\n",
    "            #print(h)\n",
    "        #else:\n",
    "        w = ws[0]\n",
    "        if w not in w2hs: w2hs[w] = []\n",
    "        if h not in w2hs[w]: w2hs[w].append(h)\n",
    "for w,hs in w2hs.items():\n",
    "    if len(hs) > 1:\n",
    "        w2hs[w] = sorted(hs)\n",
    "len(w2hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = [] \n",
    "for hs in w2hs.values():\n",
    "    if len(hs) > 1:\n",
    "        train += hs\n",
    "random.shuffle(train)\n",
    "train_set = set(train)\n",
    "\n",
    "w2ts = {} \n",
    "for w,hs in w2hs.items():\n",
    "    for h in hs:\n",
    "        if h in train_set:\n",
    "            if w not in w2ts: w2ts[w] = []\n",
    "            if h not in w2ts[w]: w2ts[w].append(h)\n",
    "for w,ts in w2ts.items(): w2ts[w] = np.array(ts)\n",
    "    \n",
    "t2i = {} \n",
    "for i,t in enumerate(train): t2i[t] = i\n",
    "\n",
    "len(train),len(w2ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "\n",
    "\n",
    "try:\n",
    "    from lap import lapjv\n",
    "    segment = False\n",
    "except ImportError:\n",
    "    print('Module lap not found, emulating with much slower scipy.optimize.linear_sum_assignment')\n",
    "    segment = True\n",
    "    from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "class TrainingData(Sequence):\n",
    "    def __init__(self, score, steps=1000, batch_size=32):\n",
    "       \n",
    "        super(TrainingData, self).__init__()\n",
    "        self.score      = -score \n",
    "        self.steps      = steps\n",
    "        self.batch_size = batch_size\n",
    "        for ts in w2ts.values():\n",
    "            idxs = [t2i[t] for t in ts]\n",
    "            for i in idxs:\n",
    "                for j in idxs:\n",
    "                    self.score[i,j] = 10000.0 \n",
    "        self.on_epoch_end()\n",
    "    def __getitem__(self, index):\n",
    "        start = self.batch_size*index\n",
    "        end   = min(start + self.batch_size, len(self.match) + len(self.unmatch))\n",
    "        size  = end - start\n",
    "        assert size > 0\n",
    "        a     = np.zeros((size,) + img_shape, dtype=K.floatx())\n",
    "        b     = np.zeros((size,) + img_shape, dtype=K.floatx())\n",
    "        c     = np.zeros((size,1), dtype=K.floatx())\n",
    "        j     = start//2\n",
    "        for i in range(0, size, 2):\n",
    "            a[i,  :,:,:] = read_for_training(self.match[j][0])\n",
    "            b[i,  :,:,:] = read_for_training(self.match[j][1])\n",
    "            c[i,  0    ] = 1 \n",
    "            a[i+1,:,:,:] = read_for_training(self.unmatch[j][0])\n",
    "            b[i+1,:,:,:] = read_for_training(self.unmatch[j][1])\n",
    "            c[i+1,0    ] = 0 \n",
    "            j           += 1\n",
    "        return [a,b],c\n",
    "    def on_epoch_end(self):\n",
    "        if self.steps <= 0: return \n",
    "        self.steps     -= 1\n",
    "        self.match      = []\n",
    "        self.unmatch    = []\n",
    "        if segment:\n",
    "            \n",
    "            tmp   = []\n",
    "            batch = 512\n",
    "            for start in range(0, score.shape[0], batch):\n",
    "                end = min(score.shape[0], start + batch)\n",
    "                _, x = linear_sum_assignment(self.score[start:end, start:end])\n",
    "                tmp.append(x + start)\n",
    "            x = np.concatenate(tmp)\n",
    "        else:\n",
    "            _,_,x = lapjv(self.score) \n",
    "        y = np.arange(len(x),dtype=np.int32)\n",
    "\n",
    "        \n",
    "        for ts in w2ts.values():\n",
    "            d = ts.copy()\n",
    "            while True:\n",
    "                random.shuffle(d)\n",
    "                if not np.any(ts == d): break\n",
    "            for ab in zip(ts,d): self.match.append(ab)\n",
    "\n",
    "       \n",
    "        for i,j in zip(x,y):\n",
    "            if i == j:\n",
    "                print(self.score)\n",
    "                print(x)\n",
    "                print(y)\n",
    "                print(i,j)\n",
    "            assert i != j\n",
    "            self.unmatch.append((train[i],train[j]))\n",
    "\n",
    "       \n",
    "        self.score[x,y] = 10000.0\n",
    "        self.score[y,x] = 10000.0\n",
    "        random.shuffle(self.match)\n",
    "        random.shuffle(self.unmatch)\n",
    "        # print(len(self.match), len(train), len(self.unmatch), len(train))\n",
    "        assert len(self.match) == len(train) and len(self.unmatch) == len(train)\n",
    "    def __len__(self):\n",
    "        return (len(self.match) + len(self.unmatch) + self.batch_size - 1)//self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = np.random.random_sample(size=(len(train),len(train)))\n",
    "data = TrainingData(score)\n",
    "(a, b), c = data[0]\n",
    "a.shape, b.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureGen(Sequence):\n",
    "    def __init__(self, data, batch_size=64, verbose=1):\n",
    "        super(FeatureGen, self).__init__()\n",
    "        self.data       = data\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose    = verbose\n",
    "        if self.verbose > 0: self.progress = tqdm_notebook(total=len(self), desc='Features')\n",
    "    def __getitem__(self, index):\n",
    "        start = self.batch_size*index\n",
    "        size  = min(len(self.data) - start, self.batch_size)\n",
    "        a     = np.zeros((size,) + img_shape, dtype=K.floatx())\n",
    "        for i in range(size): a[i,:,:,:] = read_for_validation(self.data[start + i])\n",
    "        if self.verbose > 0: \n",
    "            self.progress.update()\n",
    "            if self.progress.n >= len(self): self.progress.close()\n",
    "        return a\n",
    "    def __len__(self):\n",
    "        return (len(self.data) + self.batch_size - 1)//self.batch_size\n",
    "\n",
    "\n",
    "class ScoreGen(Sequence):\n",
    "    def __init__(self, x, y=None, batch_size=2048, verbose=1):\n",
    "        super(ScoreGen, self).__init__()\n",
    "        self.x          = x\n",
    "        self.y          = y\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose    = verbose\n",
    "        if y is None:\n",
    "            self.y           = self.x\n",
    "            self.ix, self.iy = np.triu_indices(x.shape[0],1)\n",
    "        else:\n",
    "            self.iy, self.ix = np.indices((y.shape[0],x.shape[0]))\n",
    "            self.ix          = self.ix.reshape((self.ix.size,))\n",
    "            self.iy          = self.iy.reshape((self.iy.size,))\n",
    "        self.subbatch = (len(self.x) + self.batch_size - 1)//self.batch_size\n",
    "        if self.verbose > 0: self.progress = tqdm_notebook(total=len(self), desc='Scores')\n",
    "    def __getitem__(self, index):\n",
    "        start = index*self.batch_size\n",
    "        end   = min(start + self.batch_size, len(self.ix))\n",
    "        a     = self.y[self.iy[start:end],:]\n",
    "        b     = self.x[self.ix[start:end],:]\n",
    "        if self.verbose > 0: \n",
    "            self.progress.update()\n",
    "            if self.progress.n >= len(self): self.progress.close()\n",
    "        return [a,b]\n",
    "    def __len__(self):\n",
    "        return (len(self.ix) + self.batch_size - 1)//self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "def set_lr(model, lr):\n",
    "    K.set_value(model.optimizer.lr, float(lr))\n",
    "\n",
    "def get_lr(model):\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "def score_reshape(score, x, y=None):\n",
    "   \n",
    "    if y is None:\n",
    "       \n",
    "        m = np.zeros((x.shape[0],x.shape[0]), dtype=K.floatx())\n",
    "        m[np.triu_indices(x.shape[0],1)] = score.squeeze()\n",
    "        m += m.transpose()\n",
    "    else:\n",
    "        m        = np.zeros((y.shape[0],x.shape[0]), dtype=K.floatx())\n",
    "        iy,ix    = np.indices((y.shape[0],x.shape[0]))\n",
    "        ix       = ix.reshape((ix.size,))\n",
    "        iy       = iy.reshape((iy.size,))\n",
    "        m[iy,ix] = score.squeeze()\n",
    "    return m\n",
    "\n",
    "def compute_score(verbose=1):\n",
    "   \n",
    "    features = branch_model.predict_generator(FeatureGen(train, verbose=verbose), max_queue_size=12, workers=6, verbose=0)\n",
    "    score    = head_model.predict_generator(ScoreGen(features, verbose=verbose), max_queue_size=12, workers=6, verbose=0)\n",
    "    score    = score_reshape(score, features)\n",
    "    return features, score\n",
    "\n",
    "def make_steps(step, ampl):\n",
    "    \n",
    "    global w2ts, t2i, steps, features, score, histories   \n",
    "    random.shuffle(train)\n",
    "     \n",
    "    w2ts = {}\n",
    "    for w,hs in w2hs.items():\n",
    "        for h in hs:\n",
    "            if h in train_set:\n",
    "                if w not in w2ts: w2ts[w] = []\n",
    "                if h not in w2ts[w]: w2ts[w].append(h)\n",
    "    for w,ts in w2ts.items(): w2ts[w] = np.array(ts)\n",
    "\n",
    "   \n",
    "    t2i  = {}\n",
    "    for i,t in enumerate(train): t2i[t] = i    \n",
    "    \n",
    "    features, score = compute_score()\n",
    "       \n",
    "    history = model.fit_generator(\n",
    "        TrainingData(score + ampl*np.random.random_sample(size=score.shape), steps=step, batch_size=32),#32\n",
    "        initial_epoch=steps, epochs=steps + step, max_queue_size=12, workers=6, verbose=0,\n",
    "        callbacks=[\n",
    "            TQDMNotebookCallback(leave_inner=True, metric_format='{value:0.3f}')\n",
    "        ]).history\n",
    "    steps += step\n",
    "    \n",
    "    \n",
    "    history['epochs'] = steps\n",
    "    history['ms'    ] = np.mean(score)\n",
    "    history['lr'    ] = get_lr(model)\n",
    "    print(history['epochs'],history['lr'],history['ms'])\n",
    "    histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = 'mpiotte-standard'\n",
    "histories  = []\n",
    "steps      = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if isfile('./mpiotte-standard.model'):\n",
    "    tmp = keras.models.load_model('./mpiotte-standard.model')\n",
    "    model.set_weights(tmp.get_weights())\n",
    "    print(\"model achieve\")\n",
    "    \n",
    "else:\n",
    "\n",
    "    # epoch -> 10\n",
    "    make_steps(10, 1000)\n",
    "    model.save('mpiotte-standard-10.model')\n",
    "    \n",
    "    ampl = 100.0\n",
    "    start = 10\n",
    "    for _ in range(10):\n",
    "        print('noise ampl.  = ', ampl)\n",
    "        make_steps(5, ampl)\n",
    "        start = start+5\n",
    "        model.save('mpiotte-standard-'+str(start)+'.model')\n",
    "        ampl = max(1.0, 100**-0.1*ampl)\n",
    "    #model.save('mpiotte-standard-60.model')\n",
    "    \n",
    "    # epoch -> 150\n",
    "    start = 60\n",
    "    for _ in range(18): \n",
    "        make_steps(5, 1.0)\n",
    "        start = start+5\n",
    "        model.save('mpiotte-standard-'+str(start)+'.model')\n",
    "    #model.save('mpiotte-standard-150.model')\n",
    "    \n",
    "    # epoch -> 200\n",
    "    start = 150\n",
    "    set_lr(model, 16e-5)\n",
    "    for _ in range(10): \n",
    "        make_steps(5, 0.5)\n",
    "        start = start+5\n",
    "        model.save('mpiotte-standard-'+str(start)+'.model')\n",
    "    #model.save('mpiotte-standard-200.model')\n",
    "    \n",
    "    # epoch -> 240\n",
    "    start = 200\n",
    "    set_lr(model, 4e-5)\n",
    "    for _ in range(8): \n",
    "        make_steps(5, 0.25)\n",
    "        start = start+5\n",
    "        model.save('mpiotte-standard-'+str(start)+'.model')\n",
    "    #model.save('mpiotte-standard-240.model')\n",
    "    \n",
    "    # epoch -> 280\n",
    "    start = 240\n",
    "    set_lr(model, 1e-5)\n",
    "    for _ in range(8): \n",
    "        make_steps(5, 0.25)\n",
    "        start = start+5\n",
    "        model.save('mpiotte-standard-'+str(start)+'.model')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " import gzip\n",
    "\n",
    "def prepare_submission(threshold, filename):\n",
    "    \n",
    "    vtop  = 0\n",
    "    vhigh = 0\n",
    "    pos   = [0,0,0,0,0,0]\n",
    "    \n",
    "    with gzip.open(filename, 'wt', newline='\\n') as f:\n",
    "        f.write('id,label\\n')\n",
    "        for i,p in enumerate(tqdm_notebook(submit)):\n",
    "            t = []\n",
    "            s = set()\n",
    "            a = score[i,:]\n",
    "            for j in list(reversed(np.argsort(a))):\n",
    "                h = known[j]\n",
    "                if a[j] < threshold :\n",
    "                    pos[len(t)] += 1\n",
    "                   \n",
    "                    if len(t) == 5: break;\n",
    "                for w in h2ws[h]:\n",
    "                 \n",
    "                    if w not in s:\n",
    "                        if a[j] > 1.0:\n",
    "                            vtop += 1\n",
    "                        elif a[j] >= threshold:\n",
    "                            vhigh += 1\n",
    "                        s.add(w)\n",
    "                        t.append(w)\n",
    "                        if len(t) == 5: break;\n",
    "                if len(t) == 5: break;\n",
    "           \n",
    "            assert len(t) == 5 and len(s) == 5\n",
    "            f.write(p + ',' + ' '.join(sum(t[:])/len(t)) + '\\n') #5\n",
    "    return vtop,vhigh,pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if isfile('./mpiotte-standard-280.model'):\n",
    "    tmp = keras.models.load_model('./mpiotte-standard-280.model')\n",
    "    model.set_weights(tmp.get_weights())\n",
    "    print(\"model achieve\")\n",
    "    \n",
    "    h2ws = {}\n",
    "    # 训练集\n",
    "    for p,w in tagged.items():\n",
    "        #if w != new_whale: \n",
    "        h = p2h[p]\n",
    "        if h not in h2ws: h2ws[h] = []\n",
    "        if w not in h2ws[h]: h2ws[h].append(w)\n",
    "    known = sorted(list(h2ws.keys()))\n",
    "\n",
    "   \n",
    "    h2i   = {}\n",
    "    for i,h in enumerate(known): h2i[h] = i\n",
    "\n",
    "    \n",
    "    fknown  = branch_model.predict_generator(FeatureGen(known), max_queue_size=20, workers=10, verbose=0)\n",
    "    fsubmit = branch_model.predict_generator(FeatureGen(submit), max_queue_size=20, workers=10, verbose=0)\n",
    "    score   = head_model.predict_generator(ScoreGen(fknown, fsubmit), max_queue_size=20, workers=10, verbose=0)\n",
    "    score   = score_reshape(score, fknown, fsubmit)\n",
    "    #print('score:',score)\n",
    "\n",
    "   \n",
    "    prepare_submission(0.99, 'mpiotte-standard-w0.99-280.csv.gz')\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
