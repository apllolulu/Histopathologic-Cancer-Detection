{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220025,\n",
       " 57458,\n",
       " 277483,\n",
       " [('f38a6374c348f90b587e046aac6079959adf3835', 0),\n",
       "  ('c18f2d887b7ae4f6742ee445113fa1aef383ed77', 1),\n",
       "  ('755db6279dae599ebb4d39a9123cce439965282d', 0),\n",
       "  ('bc3f0c64fb968ff4a8bd33af6971ecae77c75e08', 0),\n",
       "  ('068aba587a4950175d04c680d38943fd488d6a9d', 0)],\n",
       " ['0b2ea2a822ad23fdb1b5dd26653da899fbd2c0d5',\n",
       "  '95596b92e5066c5c52466c90b69ff089b39f2737',\n",
       "  '248e6738860e2ebcf6258cdc1f32f299e0c76914',\n",
       "  '2c35657e312966e9294eac6841726ff3a748febf',\n",
       "  '145782eb7caa1c516acbe2eda34d9a3f31c41fd6'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "tagged = dict([(p,w) for _,p,w in read_csv('../input/train_labels.csv').to_records()]) #tagged\n",
    "submit = [p for _,p,_ in read_csv('../input/sample_submission.csv').to_records()]\n",
    "join   = list(tagged.keys()) + submit\n",
    "len(tagged),len(submit),len(join),list(tagged.items())[:5],submit[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加入测试图像 第一列  mpiotte-standard-w0.99-one.csv\n",
    "#tested = dict([(p,w) for _,p,w in read_csv('./mpiotte-standard-w0.99-one.csv').to_records()])\n",
    "#list(tested.items())[:5],len(tested) #7960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tagged = tag.copy()\n",
    "#tagged.update(tested)\n",
    "# 训练集 \n",
    "#len(tagged)#33321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#join   = list(tagged.keys()) + submit\n",
    "#len(join) #41281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os.path import isfile\n",
    "from PIL import Image as pil_image\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def expand_path(p):\n",
    "    if isfile('../input/train/' + p+'.tif'): \n",
    "        #print(\"train\")\n",
    "        return '../input/train/' + p+'.tif'\n",
    "   \n",
    "    if isfile('../input/test/' + p+'.tif'): \n",
    "        #print(\"test\")\n",
    "        return '../input/test/' + p+'.tif'\n",
    "    return p\n",
    "\n",
    "p2size = {}\n",
    "for p in tqdm_notebook(join):\n",
    "    size      = pil_image.open(expand_path(p)).size\n",
    "    p2size[p] = size\n",
    "len(p2size), list(p2size.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from imagehash import phash\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "def match(h1,h2):\n",
    "    for p1 in h2ps[h1]:\n",
    "        for p2 in h2ps[h2]:\n",
    "            i1 =  pil_image.open(expand_path(p1))\n",
    "            i2 =  pil_image.open(expand_path(p2))\n",
    "            if i1.mode != i2.mode or i1.size != i2.size: return False\n",
    "            a1 = np.array(i1)\n",
    "            a1 = a1 - a1.mean()\n",
    "            a1 = a1/sqrt((a1**2).mean())\n",
    "            a2 = np.array(i2)\n",
    "            a2 = a2 - a2.mean()\n",
    "            a2 = a2/sqrt((a2**2).mean())\n",
    "            a  = ((a1 - a2)**2).mean()\n",
    "            if a > 0.1: return False\n",
    "    return True\n",
    "\n",
    "if isfile('./p2h_total.pickle'):\n",
    "    with open('./p2h_total.pickle', 'rb') as f:\n",
    "        p2h = pickle.load(f)\n",
    "else:\n",
    "    \n",
    "    p2h = {}\n",
    "    for p in tqdm_notebook(join):\n",
    "        img    = pil_image.open(expand_path(p))\n",
    "        h      = phash(img)\n",
    "        p2h[p] = h\n",
    "\n",
    "    \n",
    "    h2ps = {}\n",
    "    for p,h in p2h.items():\n",
    "        if h not in h2ps: h2ps[h] = []\n",
    "        if p not in h2ps[h]: h2ps[h].append(p)\n",
    "\n",
    "    \n",
    "    hs = list(h2ps.keys())\n",
    "\n",
    "    \n",
    "    h2h = {}\n",
    "    for i,h1 in enumerate(tqdm_notebook(hs)):\n",
    "        for h2 in hs[:i]:\n",
    "            if h1-h2 <= 6 and match(h1, h2):\n",
    "                s1 = str(h1)\n",
    "                s2 = str(h2)\n",
    "                if s1 < s2: s1,s2 = s2,s1\n",
    "                h2h[s1] = s2\n",
    "\n",
    "    \n",
    "    for p,h in p2h.items():\n",
    "        h = str(h)\n",
    "        if h in h2h: h = h2h[h]\n",
    "        p2h[p] = h\n",
    "        \n",
    "    with open('p2h_total.pickle', 'wb') as f: \n",
    "        pickle.dump(p2h, f)\n",
    "\n",
    "len(p2h), list(p2h.items())[:5] #41281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h2ps = {}\n",
    "for p,h in p2h.items():\n",
    "    if h not in h2ps: h2ps[h] = []\n",
    "    if p not in h2ps[h]: h2ps[h].append(p)\n",
    "len(h2ps),list(h2ps.items())[:5]  #33317 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_whale(imgs, per_row=2):\n",
    "    n         = len(imgs)\n",
    "    rows      = (n + per_row - 1)//per_row\n",
    "    cols      = min(per_row, n)\n",
    "    fig, axes = plt.subplots(rows,cols, figsize=(24//per_row*cols,24//per_row*rows))\n",
    "    for ax in axes.flatten(): ax.axis('off')\n",
    "    for i,(img,ax) in enumerate(zip(imgs, axes.flatten())): \n",
    "        ax.imshow(img.convert('RGB'))\n",
    "        plt.show()\n",
    "\n",
    "for h, ps in h2ps.items():\n",
    "    if len(ps) > 2:\n",
    "        print('Images:', ps)\n",
    "        imgs = [pil_image.open(expand_path(p)) for p in ps]\n",
    "        show_whale(imgs, per_row=len(ps))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prefer(ps):\n",
    "    if len(ps) == 1: return ps[0]\n",
    "    best_p = ps[0]\n",
    "    best_s = p2size[best_p]\n",
    "    for i in range(1, len(ps)):\n",
    "        p = ps[i]\n",
    "        s = p2size[p]\n",
    "        if s[0]*s[1] > best_s[0]*best_s[1]: \n",
    "            best_p = p\n",
    "            best_s = s\n",
    "    return best_p\n",
    "\n",
    "h2p = {}\n",
    "for h,ps in h2ps.items(): h2p[h] = prefer(ps)\n",
    "len(h2p),list(h2p.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./rotate.txt', 'rt') as f: rotate = f.read().split('\\n')[:-1]\n",
    "rotate = set(rotate)\n",
    "rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_raw_image(p):\n",
    "    img = pil_image.open(expand_path(p))    \n",
    "    if p in rotate: \n",
    "        img = img.rotate(180)\n",
    "        \n",
    "    return img\n",
    "\n",
    "for i in range(len(list(rotate))):\n",
    "    #print(i) \n",
    "    p    = list(rotate)[i]\n",
    "    imgs = [pil_image.open(expand_path(p)), read_raw_image(p)]\n",
    "    show_whale(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./bounding-box-new.pickle', 'rb') as f:\n",
    "    p2bb = pickle.load(f)\n",
    "list(p2bb.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "old_stderr = sys.stderr\n",
    "sys.stderr = open('/dev/null' if platform.system() != 'Windows' else 'nul', 'w')\n",
    "import keras\n",
    "sys.stderr = old_stderr\n",
    "\n",
    "import random\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import img_to_array,array_to_img\n",
    "from scipy.ndimage import affine_transform\n",
    "\n",
    "#img_shape    = (384,384,1) \n",
    "img_shape    = (512,512,1) \n",
    "anisotropy   = 2.15 \n",
    "crop_margin  = 0.05\n",
    "\n",
    "def build_transform(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    \n",
    "    rotation        = np.deg2rad(rotation)\n",
    "    shear           = np.deg2rad(shear)\n",
    "    rotation_matrix = np.array([[np.cos(rotation), np.sin(rotation), 0], [-np.sin(rotation), np.cos(rotation), 0], [0, 0, 1]])\n",
    "    shift_matrix    = np.array([[1, 0, height_shift], [0, 1, width_shift], [0, 0, 1]])\n",
    "    shear_matrix    = np.array([[1, np.sin(shear), 0], [0, np.cos(shear), 0], [0, 0, 1]])\n",
    "    zoom_matrix     = np.array([[1.0/height_zoom, 0, 0], [0, 1.0/width_zoom, 0], [0, 0, 1]])\n",
    "    shift_matrix    = np.array([[1, 0, -height_shift], [0, 1, -width_shift], [0, 0, 1]])\n",
    "    return np.dot(np.dot(rotation_matrix, shear_matrix), np.dot(zoom_matrix, shift_matrix))\n",
    "\n",
    "def read_cropped_image(p, augment):\n",
    "   \n",
    "    if p in h2p: p = h2p[p]\n",
    "    size_x,size_y = p2size[p]\n",
    "    \n",
    "   \n",
    "    x0,y0,x1,y1   = p2bb[p]\n",
    "    if p in rotate: x0, y0, x1, y1 = size_x - x1, size_y - y1, size_x - x0, size_y - y0\n",
    "    dx            = x1 - x0\n",
    "    dy            = y1 - y0\n",
    "    x0           -= dx*crop_margin\n",
    "    x1           += dx*crop_margin + 1\n",
    "    y0           -= dy*crop_margin\n",
    "    y1           += dy*crop_margin + 1\n",
    "    if (x0 < 0     ): x0 = 0\n",
    "    if (x1 > size_x): x1 = size_x\n",
    "    if (y0 < 0     ): y0 = 0\n",
    "    if (y1 > size_y): y1 = size_y\n",
    "    dx            = x1 - x0\n",
    "    dy            = y1 - y0\n",
    "    if dx > dy*anisotropy:\n",
    "        dy  = 0.5*(dx/anisotropy - dy)\n",
    "        y0 -= dy\n",
    "        y1 += dy\n",
    "    else:\n",
    "        dx  = 0.5*(dy*anisotropy - dx)\n",
    "        x0 -= dx\n",
    "        x1 += dx\n",
    "\n",
    "    \n",
    "    trans = np.array([[1, 0, -0.5*img_shape[0]], [0, 1, -0.5*img_shape[1]], [0, 0, 1]])\n",
    "    trans = np.dot(np.array([[(y1 - y0)/img_shape[0], 0, 0], [0, (x1 - x0)/img_shape[1], 0], [0, 0, 1]]), trans)\n",
    "    if augment:\n",
    "        trans = np.dot(build_transform(\n",
    "            random.uniform(-5, 5),\n",
    "            random.uniform(-5, 5),\n",
    "            random.uniform(0.8, 1.0),\n",
    "            random.uniform(0.8, 1.0),\n",
    "            random.uniform(-0.05*(y1 - y0), 0.05*(y1 - y0)),\n",
    "            random.uniform(-0.05*(x1 - x0), 0.05*(x1 - x0))\n",
    "            ), trans)\n",
    "    trans = np.dot(np.array([[1, 0, 0.5*(y1 + y0)], [0, 1, 0.5*(x1 + x0)], [0, 0, 1]]), trans)\n",
    "\n",
    "    \n",
    "    img   = read_raw_image(p).convert('L')\n",
    "    img   = img_to_array(img)\n",
    "    \n",
    "    \n",
    "    matrix = trans[:2,:2]\n",
    "    offset = trans[:2,2]\n",
    "    img    = img.reshape(img.shape[:-1])\n",
    "    img    = affine_transform(img, matrix, offset, output_shape=img_shape[:-1], order=1, mode='constant', cval=np.average(img))\n",
    "    img    = img.reshape(img_shape)\n",
    "\n",
    "   \n",
    "    img  -= np.mean(img, keepdims=True)\n",
    "    img  /= np.std(img, keepdims=True) + K.epsilon()\n",
    "    return img\n",
    "\n",
    "def read_for_training(p):\n",
    "   \n",
    "    return read_cropped_image(p, True)\n",
    "\n",
    "def read_for_validation(p):\n",
    "   \n",
    "    return read_cropped_image(p, False)\n",
    "\n",
    "p = list(tagged.keys())[312]\n",
    "imgs = [\n",
    "    read_raw_image(p),\n",
    "    array_to_img(read_for_validation(p)),\n",
    "    array_to_img(read_for_training(p))\n",
    "]\n",
    "show_whale(imgs, per_row=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "  \n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 损失\n",
    "import tensorflow as tf\n",
    "# 2分类 Dice Loss\n",
    "def dice_coefficient(y_true_cls, y_pred_cls):\n",
    "    \n",
    "    eps = 1e-5\n",
    "    intersection = tf.reduce_sum(y_true_cls * y_pred_cls )\n",
    "    union = tf.reduce_sum(y_true_cls ) + tf.reduce_sum(y_pred_cls) + eps\n",
    "    loss = 1. - (2 * intersection / union)\n",
    "\n",
    "    return loss\n",
    "\n",
    "    \n",
    "# 2 分类 focal loss\n",
    "def focal_loss(y_true, y_pred):\n",
    "    gamma=0.75\n",
    "    alpha=0.25\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    " \n",
    "    pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "    pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    " \n",
    "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "def log_dice_loss(y_true, y_pred):\n",
    "    return - K.log(dice_coefficient(y_true,y_pred))\n",
    "\n",
    "#Focal Loss + DICE LOSS\n",
    "def mixedLoss(y_true,y_pred,alpha=0.5):\n",
    "    return alpha * focal_loss(y_true,y_pred) - K.log(dice_coefficient(y_true,y_pred))\n",
    "\n",
    "\n",
    "\n",
    "def binary_crossentropy(y_true, y_pred):\n",
    "        return K.mean(K.binary_crossentropy(y_pred, y_true), axis=-1)\n",
    "\n",
    "#BCE + DICE LOSS\n",
    "def bce_logdice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) - K.log(dice_coefficient(y_true,y_pred))\n",
    "\n",
    "# WEIGHTED BCE DICE LOSS\n",
    "def weighted_dice_loss(y_true, y_pred, weight):\n",
    "    smooth = 1.\n",
    "    w, m1, m2 = weight, y_true, y_pred\n",
    "    intersection = (m1 * m2)\n",
    "    score = (2. * K.sum(w * intersection) + smooth) / (K.sum(w * m1) + K.sum(w * m2) + smooth)\n",
    "    loss = 1. - K.sum(score)\n",
    "    return loss\n",
    "\n",
    "def weighted_single_dice_loss(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    \n",
    "    averaged_mask = K.pool2d(\n",
    "            y_true, pool_size=(50, 50), strides=(1, 1), padding='same', pool_mode='avg')\n",
    "    weight = K.ones_like(averaged_mask)\n",
    "    w0 = K.sum(weight)\n",
    "    weight = 5. * K.exp(-5. * K.abs(averaged_mask - 0.5))\n",
    "    w1 = K.sum(weight)\n",
    "    weight *= (w0 / w1)\n",
    "    loss = weighted_dice_loss(y_true, y_pred, weight)\n",
    "    return loss\n",
    "\n",
    "# weighted_log_dice_loss\n",
    "def weighted_log_dice_loss(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    \n",
    "    averaged_mask = K.pool2d(\n",
    "            y_true, pool_size=(50, 50), strides=(1, 1), padding='same', pool_mode='avg')\n",
    "    weight = K.ones_like(averaged_mask)\n",
    "    w0 = K.sum(weight)\n",
    "    weight = 5. * K.exp(-5. * K.abs(averaged_mask - 0.5))\n",
    "    w1 = K.sum(weight)\n",
    "    weight *= (w0 / w1)\n",
    "    loss = - K.log(weighted_dice_loss(y_true, y_pred, weight))\n",
    "    return loss\n",
    "\n",
    "def weighted_focal_dice_loss(y_true, y_pred,alpha=1):\n",
    "    \n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    \n",
    "    averaged_mask = K.pool2d(\n",
    "            y_true, pool_size=(50, 50), strides=(1, 1), padding='same', pool_mode='avg')\n",
    "    weight = K.ones_like(averaged_mask)\n",
    "    w0 = K.sum(weight)\n",
    "    weight = 5. * K.exp(-5. * K.abs(averaged_mask - 0.5))\n",
    "    w1 = K.sum(weight)\n",
    "    weight *= (w0 / w1)\n",
    "    loss = weighted_dice_loss(y_true, y_pred, weight) + alpha*focal_loss(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.engine.topology import Input\n",
    "from keras.layers import Activation, Add, BatchNormalization, Concatenate, Conv2D, Dense, Flatten, GlobalMaxPooling2D, Lambda, MaxPooling2D, Reshape\n",
    "from keras.models import Model\n",
    "\n",
    "def subblock(x, filter, **kwargs):\n",
    "    x = BatchNormalization()(x)\n",
    "    y = x\n",
    "    y = Conv2D(filter, (1, 1), activation='relu', **kwargs)(y) \n",
    "    y = BatchNormalization()(y)\n",
    "    y = Conv2D(filter, (3, 3), activation='relu', **kwargs)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Conv2D(K.int_shape(x)[-1], (1, 1), **kwargs)(y)\n",
    "    y = Add()([x,y]) \n",
    "    y = Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "def build_model(lr, l2, activation='sigmoid'):\n",
    "\n",
    "   \n",
    "    regul  = regularizers.l2(l2)\n",
    "    optim  = Adam(lr=lr)\n",
    "    kwargs = {'padding':'same', 'kernel_regularizer':regul}\n",
    "\n",
    "    inp = Input(shape=img_shape) # 384x384x1\n",
    "    x   = Conv2D(64, (9,9), strides=2, activation='relu', **kwargs)(inp)\n",
    "\n",
    "    x   = MaxPooling2D((2, 2), strides=(2, 2))(x) # 96x96x64\n",
    "    for _ in range(2):\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2D(64, (3,3), activation='relu', **kwargs)(x)\n",
    "\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x) # 48x48x64\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, (1,1), activation='relu', **kwargs)(x) # 48x48x128\n",
    "    for _ in range(4): x = subblock(x, 64, **kwargs)\n",
    "\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x) # 24x24x128\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(256, (1,1), activation='relu', **kwargs)(x) # 24x24x256\n",
    "    for _ in range(4): x = subblock(x, 64, **kwargs)\n",
    "\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x) # 12x12x256\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(384, (1,1), activation='relu', **kwargs)(x) # 12x12x384\n",
    "    for _ in range(4): x = subblock(x, 96, **kwargs)\n",
    "\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x) # 6x6x384\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(512, (1,1), activation='relu', **kwargs)(x) # 6x6x512\n",
    "    for _ in range(4): x = subblock(x, 128, **kwargs)\n",
    "    \n",
    "    x             = GlobalMaxPooling2D()(x) # 512\n",
    "    branch_model  = Model(inp, x)\n",
    "    \n",
    "   \n",
    "    mid        = 32\n",
    "    xa_inp     = Input(shape=branch_model.output_shape[1:])\n",
    "    xb_inp     = Input(shape=branch_model.output_shape[1:])\n",
    "    x1         = Lambda(lambda x : x[0]*x[1])([xa_inp, xb_inp])\n",
    "    x2         = Lambda(lambda x : x[0] + x[1])([xa_inp, xb_inp])\n",
    "    x3         = Lambda(lambda x : K.abs(x[0] - x[1]))([xa_inp, xb_inp])\n",
    "    x4         = Lambda(lambda x : K.square(x))(x3)\n",
    "    x          = Concatenate()([x1, x2, x3, x4])\n",
    "    x          = Reshape((4, branch_model.output_shape[1], 1), name='reshape1')(x)\n",
    "\n",
    "    \n",
    "    x          = Conv2D(mid, (4, 1), activation='relu', padding='valid')(x)\n",
    "    x          = Reshape((branch_model.output_shape[1], mid, 1))(x)\n",
    "    x          = Conv2D(1, (1, mid), activation='linear', padding='valid')(x)\n",
    "    x          = Flatten(name='flatten')(x)\n",
    "    \n",
    "    \n",
    "    x          = Dense(1, use_bias=True, activation=activation, name='weighted-average')(x)\n",
    "    head_model = Model([xa_inp, xb_inp], x, name='head')\n",
    "    \n",
    "    img_a      = Input(shape=img_shape)\n",
    "    img_b      = Input(shape=img_shape)\n",
    "    xa         = branch_model(img_a)\n",
    "    xb         = branch_model(img_b)\n",
    "    x          = head_model([xa, xb])\n",
    "    model      = Model([img_a, img_b], x)\n",
    "    model.compile(optim, loss='binary_crossentropy', metrics=['binary_crossentropy', 'acc'])\n",
    "    #model.compile(optim, loss=mixedLoss, metrics=[matthews_correlation, 'acc']) \n",
    "    return model, branch_model, head_model\n",
    "\n",
    "model, branch_model, head_model = build_model(64e-5,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "branch_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "head_model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(head_model, to_file='head-model.png')\n",
    "pil_image.open('head-model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_model(branch_model, to_file='branch-model.png')\n",
    "img = pil_image.open('branch-model.png')\n",
    "img.resize([x//2 for x in img.size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./exclude.txt', 'rt') as f: exclude = f.read().split('\\n')[:-1]   \n",
    "len(exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs=[]\n",
    "for p in exclude:\n",
    "    img = read_raw_image(p)\n",
    "    imgs.append(img)\n",
    "    \n",
    "#len(imgs)    \n",
    "show_whale(imgs, per_row=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h2ws = {}\n",
    "new_whale = 'new_whale'\n",
    "for p,w in tagged.items():\n",
    "    if w != new_whale: \n",
    "        h = p2h[p]\n",
    "        if h not in h2ws: h2ws[h] = []\n",
    "        if w not in h2ws[h]: h2ws[h].append(w)\n",
    "for h,ws in h2ws.items():\n",
    "    if len(ws) > 1:\n",
    "        h2ws[h] = sorted(ws)\n",
    "len(h2ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2hs = {}\n",
    "for h,ws in h2ws.items():\n",
    "    if len(ws) == 1: \n",
    "        if h2p[h] in exclude:\n",
    "            print(h)\n",
    "        else:\n",
    "            w = ws[0]\n",
    "            if w not in w2hs: w2hs[w] = []\n",
    "            if h not in w2hs[w]: w2hs[w].append(h)\n",
    "for w,hs in w2hs.items():\n",
    "    if len(hs) > 1:\n",
    "        w2hs[w] = sorted(hs)\n",
    "len(w2hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = [] \n",
    "for hs in w2hs.values():\n",
    "    if len(hs) > 1:\n",
    "        train += hs\n",
    "random.shuffle(train)\n",
    "train_set = set(train)\n",
    "\n",
    "w2ts = {} \n",
    "for w,hs in w2hs.items():\n",
    "    for h in hs:\n",
    "        if h in train_set:\n",
    "            if w not in w2ts: w2ts[w] = []\n",
    "            if h not in w2ts[w]: w2ts[w].append(h)\n",
    "for w,ts in w2ts.items(): w2ts[w] = np.array(ts)\n",
    "    \n",
    "t2i = {} \n",
    "for i,t in enumerate(train): t2i[t] = i\n",
    "\n",
    "len(train),len(w2ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "\n",
    "\n",
    "try:\n",
    "    from lap import lapjv\n",
    "    segment = False\n",
    "except ImportError:\n",
    "    print('Module lap not found, emulating with much slower scipy.optimize.linear_sum_assignment')\n",
    "    segment = True\n",
    "    from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "class TrainingData(Sequence):\n",
    "    def __init__(self, score, steps=1000, batch_size=32):\n",
    "       \n",
    "        super(TrainingData, self).__init__()\n",
    "        self.score      = -score \n",
    "        self.steps      = steps\n",
    "        self.batch_size = batch_size\n",
    "        for ts in w2ts.values():\n",
    "            idxs = [t2i[t] for t in ts]\n",
    "            for i in idxs:\n",
    "                for j in idxs:\n",
    "                    self.score[i,j] = 10000.0 \n",
    "        self.on_epoch_end()\n",
    "    def __getitem__(self, index):\n",
    "        start = self.batch_size*index\n",
    "        end   = min(start + self.batch_size, len(self.match) + len(self.unmatch))\n",
    "        size  = end - start\n",
    "        assert size > 0\n",
    "        a     = np.zeros((size,) + img_shape, dtype=K.floatx())\n",
    "        b     = np.zeros((size,) + img_shape, dtype=K.floatx())\n",
    "        c     = np.zeros((size,1), dtype=K.floatx())\n",
    "        j     = start//2\n",
    "        for i in range(0, size, 2):\n",
    "            a[i,  :,:,:] = read_for_training(self.match[j][0])\n",
    "            b[i,  :,:,:] = read_for_training(self.match[j][1])\n",
    "            c[i,  0    ] = 1 \n",
    "            a[i+1,:,:,:] = read_for_training(self.unmatch[j][0])\n",
    "            b[i+1,:,:,:] = read_for_training(self.unmatch[j][1])\n",
    "            c[i+1,0    ] = 0 \n",
    "            j           += 1\n",
    "        return [a,b],c\n",
    "    def on_epoch_end(self):\n",
    "        if self.steps <= 0: return \n",
    "        self.steps     -= 1\n",
    "        self.match      = []\n",
    "        self.unmatch    = []\n",
    "        if segment:\n",
    "            \n",
    "            tmp   = []\n",
    "            batch = 512\n",
    "            for start in range(0, score.shape[0], batch):\n",
    "                end = min(score.shape[0], start + batch)\n",
    "                _, x = linear_sum_assignment(self.score[start:end, start:end])\n",
    "                tmp.append(x + start)\n",
    "            x = np.concatenate(tmp)\n",
    "        else:\n",
    "            _,_,x = lapjv(self.score) \n",
    "        y = np.arange(len(x),dtype=np.int32)\n",
    "\n",
    "        \n",
    "        for ts in w2ts.values():\n",
    "            d = ts.copy()\n",
    "            while True:\n",
    "                random.shuffle(d)\n",
    "                if not np.any(ts == d): break\n",
    "            for ab in zip(ts,d): self.match.append(ab)\n",
    "\n",
    "       \n",
    "        for i,j in zip(x,y):\n",
    "            if i == j:\n",
    "                print(self.score)\n",
    "                print(x)\n",
    "                print(y)\n",
    "                print(i,j)\n",
    "            assert i != j\n",
    "            self.unmatch.append((train[i],train[j]))\n",
    "\n",
    "       \n",
    "        self.score[x,y] = 10000.0\n",
    "        self.score[y,x] = 10000.0\n",
    "        random.shuffle(self.match)\n",
    "        random.shuffle(self.unmatch)\n",
    "        # print(len(self.match), len(train), len(self.unmatch), len(train))\n",
    "        assert len(self.match) == len(train) and len(self.unmatch) == len(train)\n",
    "    def __len__(self):\n",
    "        return (len(self.match) + len(self.unmatch) + self.batch_size - 1)//self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = np.random.random_sample(size=(len(train),len(train)))\n",
    "data = TrainingData(score)\n",
    "(a, b), c = data[0]\n",
    "a.shape, b.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs = [array_to_img(a[0]), array_to_img(b[0])]\n",
    "show_whale(imgs, per_row=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs = [array_to_img(a[1]), array_to_img(b[1])]\n",
    "show_whale(imgs, per_row=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureGen(Sequence):\n",
    "    def __init__(self, data, batch_size=64, verbose=1):\n",
    "        super(FeatureGen, self).__init__()\n",
    "        self.data       = data\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose    = verbose\n",
    "        if self.verbose > 0: self.progress = tqdm_notebook(total=len(self), desc='Features')\n",
    "    def __getitem__(self, index):\n",
    "        start = self.batch_size*index\n",
    "        size  = min(len(self.data) - start, self.batch_size)\n",
    "        a     = np.zeros((size,) + img_shape, dtype=K.floatx())\n",
    "        for i in range(size): a[i,:,:,:] = read_for_validation(self.data[start + i])\n",
    "        if self.verbose > 0: \n",
    "            self.progress.update()\n",
    "            if self.progress.n >= len(self): self.progress.close()\n",
    "        return a\n",
    "    def __len__(self):\n",
    "        return (len(self.data) + self.batch_size - 1)//self.batch_size\n",
    "\n",
    "\n",
    "class ScoreGen(Sequence):\n",
    "    def __init__(self, x, y=None, batch_size=2048, verbose=1):\n",
    "        super(ScoreGen, self).__init__()\n",
    "        self.x          = x\n",
    "        self.y          = y\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose    = verbose\n",
    "        if y is None:\n",
    "            self.y           = self.x\n",
    "            self.ix, self.iy = np.triu_indices(x.shape[0],1)\n",
    "        else:\n",
    "            self.iy, self.ix = np.indices((y.shape[0],x.shape[0]))\n",
    "            self.ix          = self.ix.reshape((self.ix.size,))\n",
    "            self.iy          = self.iy.reshape((self.iy.size,))\n",
    "        self.subbatch = (len(self.x) + self.batch_size - 1)//self.batch_size\n",
    "        if self.verbose > 0: self.progress = tqdm_notebook(total=len(self), desc='Scores')\n",
    "    def __getitem__(self, index):\n",
    "        start = index*self.batch_size\n",
    "        end   = min(start + self.batch_size, len(self.ix))\n",
    "        a     = self.y[self.iy[start:end],:]\n",
    "        b     = self.x[self.ix[start:end],:]\n",
    "        if self.verbose > 0: \n",
    "            self.progress.update()\n",
    "            if self.progress.n >= len(self): self.progress.close()\n",
    "        return [a,b]\n",
    "    def __len__(self):\n",
    "        return (len(self.ix) + self.batch_size - 1)//self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "def set_lr(model, lr):\n",
    "    K.set_value(model.optimizer.lr, float(lr))\n",
    "\n",
    "def get_lr(model):\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "def score_reshape(score, x, y=None):\n",
    "   \n",
    "    if y is None:\n",
    "       \n",
    "        m = np.zeros((x.shape[0],x.shape[0]), dtype=K.floatx())\n",
    "        m[np.triu_indices(x.shape[0],1)] = score.squeeze()\n",
    "        m += m.transpose()\n",
    "    else:\n",
    "        m        = np.zeros((y.shape[0],x.shape[0]), dtype=K.floatx())\n",
    "        iy,ix    = np.indices((y.shape[0],x.shape[0]))\n",
    "        ix       = ix.reshape((ix.size,))\n",
    "        iy       = iy.reshape((iy.size,))\n",
    "        m[iy,ix] = score.squeeze()\n",
    "    return m\n",
    "\n",
    "def compute_score(verbose=1):\n",
    "   \n",
    "    features = branch_model.predict_generator(FeatureGen(train, verbose=verbose), max_queue_size=12, workers=6, verbose=0)\n",
    "    score    = head_model.predict_generator(ScoreGen(features, verbose=verbose), max_queue_size=12, workers=6, verbose=0)\n",
    "    score    = score_reshape(score, features)\n",
    "    return features, score\n",
    "\n",
    "def make_steps(step, ampl):\n",
    "    \n",
    "    global w2ts, t2i, steps, features, score, histories\n",
    "    \n",
    "    \n",
    "    random.shuffle(train)\n",
    "    \n",
    "  \n",
    "    w2ts = {}\n",
    "    for w,hs in w2hs.items():\n",
    "        for h in hs:\n",
    "            if h in train_set:\n",
    "                if w not in w2ts: w2ts[w] = []\n",
    "                if h not in w2ts[w]: w2ts[w].append(h)\n",
    "    for w,ts in w2ts.items(): w2ts[w] = np.array(ts)\n",
    "\n",
    "   \n",
    "    t2i  = {}\n",
    "    for i,t in enumerate(train): t2i[t] = i    \n",
    "\n",
    "    \n",
    "    features, score = compute_score()\n",
    "    \n",
    "   \n",
    "    history = model.fit_generator(\n",
    "        TrainingData(score + ampl*np.random.random_sample(size=score.shape), steps=step, batch_size=32),#32\n",
    "        initial_epoch=steps, epochs=steps + step, max_queue_size=12, workers=6, verbose=0,\n",
    "        callbacks=[\n",
    "            TQDMNotebookCallback(leave_inner=True, metric_format='{value:0.3f}')\n",
    "        ]).history\n",
    "    steps += step\n",
    "    \n",
    "    \n",
    "    history['epochs'] = steps\n",
    "    history['ms'    ] = np.mean(score)\n",
    "    history['lr'    ] = get_lr(model)\n",
    "    print(history['epochs'],history['lr'],history['ms'])\n",
    "    histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = 'mpiotte-standard'\n",
    "histories  = []\n",
    "steps      = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if isfile('./mpiotte-standard.model'):\n",
    "    tmp = keras.models.load_model('./mpiotte-standard.model')\n",
    "    model.set_weights(tmp.get_weights())\n",
    "    print(\"model achieve\")\n",
    "    \n",
    "else:\n",
    "    \n",
    "    tmp = keras.models.load_model('./mpiotte-standard-400.model')\n",
    "    model.set_weights(tmp.get_weights())\n",
    "    print(\"mpiotte-standard-400.model success\")\n",
    "    \n",
    "    \n",
    "    start = 400\n",
    "    set_lr(model, 1e-5)#1e-5\n",
    "    for _ in range(8): \n",
    "        #print(\"start:\",start)\n",
    "        make_steps(5, 0.25)\n",
    "        start = start+5      \n",
    "        model.save('mpiotte-standard-'+str(start)+'.model')       \n",
    "   \n",
    "\n",
    "\"\"\"\n",
    "    # epoch -> 10\n",
    "    make_steps(10, 1000)\n",
    "    model.save('mpiotte-standard-10.model')\n",
    "    ampl = 100.0\n",
    "    for _ in range(10):\n",
    "        print('noise ampl.  = ', ampl)\n",
    "        make_steps(5, ampl)\n",
    "        ampl = max(1.0, 100**-0.1*ampl)\n",
    "    model.save('mpiotte-standard-60.model')\n",
    "    \n",
    "    # epoch -> 150\n",
    "    for _ in range(18): make_steps(5, 1.0)\n",
    "    model.save('mpiotte-standard-150.model')\n",
    "    # epoch -> 200\n",
    "    set_lr(model, 16e-5)\n",
    "    for _ in range(10): make_steps(5, 0.5)\n",
    "    model.save('mpiotte-standard-200.model')\n",
    "    # epoch -> 240\n",
    "    set_lr(model, 4e-5)\n",
    "    for _ in range(8): make_steps(5, 0.25)\n",
    "    model.save('mpiotte-standard-240.model')\n",
    "    # epoch -> 250\n",
    "    set_lr(model, 1e-5)\n",
    "    for _ in range(2): make_steps(5, 0.25)\n",
    "    model.save('mpiotte-standard-250.model')\n",
    "   \n",
    "    # epoch -> 240\n",
    "    set_lr(model, 4e-5)\n",
    "    start = 205\n",
    "    for _ in range(8): \n",
    "        print(\"start:\",start)\n",
    "        make_steps(5, 0.25)\n",
    "        start = start+5\n",
    "        \n",
    "        model.save('mpiotte-standard-'+str(start)+'.model')\n",
    "    \n",
    "    # epoch -> 300\n",
    "    weights = model.get_weights()\n",
    "    model, branch_model, head_model = build_model(64e-5,0.0002)\n",
    "    model.set_weights(weights)\n",
    "    for _ in range(10): make_steps(5, 1.0)\n",
    "    model.save('mpiotte-standard-300.model')\n",
    "    # epoch -> 350\n",
    "    set_lr(model, 16e-5)\n",
    "    for _ in range(10): make_steps(5, 0.5) \n",
    "    model.save('mpiotte-standard-350.model') \n",
    "\n",
    "    \n",
    "    tmp = keras.models.load_model('./mpiotte-standard-total-440.model')\n",
    "    model.set_weights(tmp.get_weights())\n",
    "    print(\"mpiotte-standard-total-440 success\")\n",
    "    \n",
    " \n",
    "    # epoch -> 440\n",
    "    start = 400\n",
    "    set_lr(model, 5e-6)#1e-5\n",
    "    for _ in range(8): \n",
    "        #print(\"start:\",start)\n",
    "        make_steps(5, 0.25)\n",
    "        start = start+5      \n",
    "        model.save('mpiotte-standard-new-'+str(start)+'.model')       \n",
    "   \n",
    "    \n",
    "    # epoch -> 490\n",
    "    start = 455\n",
    "    set_lr(model, 1e-5)\n",
    "    for _ in range(10): \n",
    "        #print(\"start:\",start)\n",
    "        make_steps(5, 0.25)\n",
    "        start = start+5      \n",
    "        model.save('mpiotte-standard-total-'+str(start)+'.model')\n",
    "    \n",
    "    \n",
    "    #for _ in range(8): make_steps(5, 0.25)\n",
    "    #model.save('mpiotte-standard-390.model')   \n",
    "    # epoch -> 400\n",
    "   # set_lr(model, 1e-5)\n",
    "   # for _ in range(2): make_steps(5, 0.25)\n",
    "  #  model.save('mpiotte-standard-400.model')\n",
    "  #  model.save('mpiotte-standard.model')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "def prepare_submission(threshold, filename):\n",
    "    \n",
    "    vtop  = 0\n",
    "    vhigh = 0\n",
    "    pos   = [0,0,0,0,0,0]\n",
    "    with gzip.open(filename, 'wt', newline='\\n') as f:\n",
    "        f.write('Image,Id\\n')\n",
    "        for i,p in enumerate(tqdm_notebook(submit)):\n",
    "            t = []\n",
    "            s = set()\n",
    "            a = score[i,:]\n",
    "            for j in list(reversed(np.argsort(a))):\n",
    "                h = known[j]\n",
    "                if a[j] < threshold and new_whale not in s:\n",
    "                    pos[len(t)] += 1\n",
    "                    s.add(new_whale)\n",
    "                    t.append(new_whale)\n",
    "                    if len(t) == 5: break;\n",
    "                for w in h2ws[h]:\n",
    "                    assert w != new_whale\n",
    "                    if w not in s:\n",
    "                        if a[j] > 1.0:\n",
    "                            vtop += 1\n",
    "                        elif a[j] >= threshold:\n",
    "                            vhigh += 1\n",
    "                        s.add(w)\n",
    "                        t.append(w)\n",
    "                        if len(t) == 5: break;\n",
    "                if len(t) == 5: break;\n",
    "            if new_whale not in s: pos[5] += 1\n",
    "            assert len(t) == 5 and len(s) == 5\n",
    "            f.write(p + ',' + ' '.join(t[:5]) + '\\n') #5\n",
    "    return vtop,vhigh,pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if isfile('./mpiotte-standard-total-445.model'):\n",
    "    tmp = keras.models.load_model('./mpiotte-standard-total-445.model')\n",
    "    model.set_weights(tmp.get_weights())\n",
    "    print(\"model achieve\")\n",
    "    \n",
    "    h2ws = {}\n",
    "    # 训练集\n",
    "    for p,w in tagged.items():\n",
    "        if w != new_whale: \n",
    "            h = p2h[p]\n",
    "            if h not in h2ws: h2ws[h] = []\n",
    "            if w not in h2ws[h]: h2ws[h].append(w)\n",
    "    known = sorted(list(h2ws.keys()))\n",
    "\n",
    "   \n",
    "    h2i   = {}\n",
    "    for i,h in enumerate(known): h2i[h] = i\n",
    "\n",
    "    \n",
    "    fknown  = branch_model.predict_generator(FeatureGen(known), max_queue_size=20, workers=10, verbose=0)\n",
    "    fsubmit = branch_model.predict_generator(FeatureGen(submit), max_queue_size=20, workers=10, verbose=0)\n",
    "    score   = head_model.predict_generator(ScoreGen(fknown, fsubmit), max_queue_size=20, workers=10, verbose=0)\n",
    "    score   = score_reshape(score, fknown, fsubmit)\n",
    "    #print('score:',score)\n",
    "\n",
    "   \n",
    "    prepare_submission(0.92, 'mpiotte-standard-w0.92-total-445.csv.gz')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if isfile('./mpiotte-standard-total-450.model'):\n",
    "    tmp = keras.models.load_model('./mpiotte-standard-total-450.model')\n",
    "    model.set_weights(tmp.get_weights())\n",
    "    print(\"model achieve\")\n",
    "    \n",
    "    h2ws = {}\n",
    "    # 训练集\n",
    "    for p,w in tagged.items():\n",
    "        if w != new_whale: \n",
    "            h = p2h[p]\n",
    "            if h not in h2ws: h2ws[h] = []\n",
    "            if w not in h2ws[h]: h2ws[h].append(w)\n",
    "    known = sorted(list(h2ws.keys()))\n",
    "\n",
    "   \n",
    "    h2i   = {}\n",
    "    for i,h in enumerate(known): h2i[h] = i\n",
    "\n",
    "    \n",
    "    fknown  = branch_model.predict_generator(FeatureGen(known), max_queue_size=20, workers=10, verbose=0)\n",
    "    fsubmit = branch_model.predict_generator(FeatureGen(submit), max_queue_size=20, workers=10, verbose=0)\n",
    "    score   = head_model.predict_generator(ScoreGen(fknown, fsubmit), max_queue_size=20, workers=10, verbose=0)\n",
    "    score   = score_reshape(score, fknown, fsubmit)\n",
    "    #print('score:',score)\n",
    "\n",
    "   \n",
    "    prepare_submission(0.92, 'mpiotte-standard-w0.92-total-450.csv.gz')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if isfile('./mpiotte-standard-total-455.model'):\n",
    "    tmp = keras.models.load_model('./mpiotte-standard-total-455.model')\n",
    "    model.set_weights(tmp.get_weights())\n",
    "    print(\"model achieve\")\n",
    "    \n",
    "    h2ws = {}\n",
    "    # 训练集\n",
    "    for p,w in tagged.items():\n",
    "        if w != new_whale: \n",
    "            h = p2h[p]\n",
    "            if h not in h2ws: h2ws[h] = []\n",
    "            if w not in h2ws[h]: h2ws[h].append(w)\n",
    "    known = sorted(list(h2ws.keys()))\n",
    "\n",
    "   \n",
    "    h2i   = {}\n",
    "    for i,h in enumerate(known): h2i[h] = i\n",
    "\n",
    "    \n",
    "    fknown  = branch_model.predict_generator(FeatureGen(known), max_queue_size=20, workers=10, verbose=0)\n",
    "    fsubmit = branch_model.predict_generator(FeatureGen(submit), max_queue_size=20, workers=10, verbose=0)\n",
    "    score   = head_model.predict_generator(ScoreGen(fknown, fsubmit), max_queue_size=20, workers=10, verbose=0)\n",
    "    score   = score_reshape(score, fknown, fsubmit)\n",
    "    #print('score:',score)\n",
    "\n",
    "   \n",
    "    prepare_submission(0.92, 'mpiotte-standard-w0.92-total-455.csv.gz')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if isfile('./mpiotte-standard.model'):\n",
    "    tmp = keras.models.load_model('./mpiotte-standard.model')\n",
    "    model.set_weights(tmp.get_weights())\n",
    "    print(\"model achieve\")\n",
    "    \n",
    "else:\n",
    "    \"\"\"\n",
    "    # epoch -> 10\n",
    "    make_steps(10, 1000)\n",
    "    model.save('mpiotte-standard-10.model')\n",
    "    ampl = 100.0\n",
    "    for _ in range(10):\n",
    "        print('noise ampl.  = ', ampl)\n",
    "        make_steps(5, ampl)\n",
    "        ampl = max(1.0, 100**-0.1*ampl)\n",
    "    model.save('mpiotte-standard-60.model')\n",
    "    \n",
    "    # epoch -> 150\n",
    "    for _ in range(18): make_steps(5, 1.0)\n",
    "    model.save('mpiotte-standard-150.model')\n",
    "    # epoch -> 200\n",
    "    set_lr(model, 16e-5)\n",
    "    for _ in range(10): make_steps(5, 0.5)\n",
    "    model.save('mpiotte-standard-200.model')\n",
    "    # epoch -> 240\n",
    "    set_lr(model, 4e-5)\n",
    "    for _ in range(8): make_steps(5, 0.25)\n",
    "    model.save('mpiotte-standard-240.model')\n",
    "    # epoch -> 250\n",
    "    set_lr(model, 1e-5)\n",
    "    for _ in range(2): make_steps(5, 0.25)\n",
    "    model.save('mpiotte-standard-250.model')\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "       \n",
    "    # epoch -> 240\n",
    "    set_lr(model, 4e-5)\n",
    "    start = 205\n",
    "    for _ in range(8): \n",
    "        print(\"start:\",start)\n",
    "        make_steps(5, 0.25)\n",
    "        start = start+5\n",
    "        \n",
    "        model.save('mpiotte-standard-'+str(start)+'.model')\n",
    "    \n",
    "    # epoch -> 300\n",
    "    weights = model.get_weights()\n",
    "    model, branch_model, head_model = build_model(64e-5,0.0002)\n",
    "    model.set_weights(weights)\n",
    "    for _ in range(10): make_steps(5, 1.0)\n",
    "    model.save('mpiotte-standard-300.model')\n",
    "    # epoch -> 350\n",
    "    set_lr(model, 16e-5)\n",
    "    for _ in range(10): make_steps(5, 0.5) \n",
    "    model.save('mpiotte-standard-350.model') \n",
    "    \"\"\"\n",
    "    \n",
    "    tmp = keras.models.load_model('./mpiotte-standard-total-440.model')\n",
    "    model.set_weights(tmp.get_weights())\n",
    "    print(\"mpiotte-standard-total-440 success\")\n",
    "    \n",
    "    \"\"\"\n",
    "    # epoch -> 440\n",
    "    start = 400\n",
    "    set_lr(model, 5e-6)#1e-5\n",
    "    for _ in range(8): \n",
    "        #print(\"start:\",start)\n",
    "        make_steps(5, 0.25)\n",
    "        start = start+5      \n",
    "        model.save('mpiotte-standard-new-'+str(start)+'.model')       \n",
    "    \"\"\"\n",
    "    \n",
    "    # epoch -> 490\n",
    "    start = 455\n",
    "    set_lr(model, 1e-5)\n",
    "    for _ in range(10): \n",
    "        #print(\"start:\",start)\n",
    "        make_steps(5, 0.25)\n",
    "        start = start+5      \n",
    "        model.save('mpiotte-standard-total-'+str(start)+'.model')\n",
    "    \n",
    "    \n",
    "    #for _ in range(8): make_steps(5, 0.25)\n",
    "    #model.save('mpiotte-standard-390.model')   \n",
    "    # epoch -> 400\n",
    "   # set_lr(model, 1e-5)\n",
    "   # for _ in range(2): make_steps(5, 0.25)\n",
    "  #  model.save('mpiotte-standard-400.model')\n",
    "  #  model.save('mpiotte-standard.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./bootstrap.pickle', 'rb') as f:\n",
    "    bootstrap = pickle.load(f)\n",
    "len(bootstrap), list(bootstrap.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = head_model.layers[-1].get_weights()[0]\n",
    "w = w.flatten().tolist()\n",
    "w = sorted(w)\n",
    "fig, axes = plt.subplots(1,1)\n",
    "axes.bar(range(len(w)), w)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, _, tmp_model = build_model(64e-5,0, activation='linear')\n",
    "tmp_model.set_weights(head_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.ones((21*21,512),dtype=K.floatx())\n",
    "b = np.ones((21*21,512),dtype=K.floatx())\n",
    "for i in range(21):\n",
    "    for j in range(21):\n",
    "        a[21*i + j] *= float(i)/10.0\n",
    "        b[21*i + j] *= float(j)/10.0\n",
    "x    = np.arange(0.0, 2.01, 0.1, dtype=K.floatx())\n",
    "x, y = np.meshgrid(x, x)\n",
    "z    = tmp_model.predict([a,b], verbose=0).reshape((21,21))\n",
    "x.shape, y.shape, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(x, y, z, cmap=cm.coolwarm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import BoundaryNorm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "levels = MaxNLocator(nbins=15).tick_values(z.min(), z.max())\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cf = ax.contourf(x, y, z, levels=levels, cmap=cm.coolwarm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "def show_filter(filter, blur):\n",
    "    np.random.seed(1)\n",
    "    noise   = 0.1 \n",
    "    step    = 1 \n",
    "    \n",
    "    \n",
    "    inp     = branch_model.layers[0].get_input_at(0)\n",
    "    loss    = K.mean(branch_model.layers[-3].output[0,2:4,2:4,filter]) \n",
    "    grads   = K.gradients(loss, inp)[0]\n",
    "    grads  /= K.sqrt(K.mean(K.square(grads))) + K.epsilon()\n",
    "    iterate = K.function([inp],[grads])\n",
    "    img     = (np.random.random(img_shape) -0.5)*noise\n",
    "    img     = np.expand_dims(img, 0)\n",
    "\n",
    "   \n",
    "    for i in range(200):\n",
    "        grads_value = iterate([img])[0]\n",
    "        \n",
    "        img = gaussian_filter(img + grads_value*step, sigma=blur)\n",
    "\n",
    "  \n",
    "    avg  = np.mean(img)\n",
    "    std  = sqrt(np.mean((img - avg)**2))\n",
    "    low  = avg - 5*std\n",
    "    high = avg + 5*std\n",
    "    return array_to_img(np.minimum(high, np.maximum(low, img))[0])\n",
    "\n",
    "\n",
    "show_whale([show_filter(i, 0.5) for i in tqdm_notebook(range(25))], per_row=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
